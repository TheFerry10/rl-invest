{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investment Game\n",
    "The rules of the investment game are outlined in the `README.md`. Here we are implementing a simple environment in order to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import operator\n",
    "import ujson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvestmentGameSimple(gym.Env):\n",
    "    def __init__(self):\n",
    "        # add transaction costs\n",
    "        file_name = 'resources/data/asset.csv'\n",
    "        self.asset = pd.read_csv(file_name)\n",
    "        self.num_rounds = len(self.asset)\n",
    "        self.initial_balance = 1\n",
    "        self.action_space = ['buy', 'sell', 'hold'] # depending on state\n",
    "        self.price_levels = np.array([4,7,10])\n",
    "        self.state_space = list(itertools.product(self.price_levels, [-1, 1], [True, False]))\n",
    "        \n",
    "    def is_terminal(self):\n",
    "        if self.round >= self.num_rounds:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def transform_price_to_price_level(self, price):\n",
    "        return self.price_levels[np.argmin(np.abs(self.price_levels - price))]\n",
    "        \n",
    "    def reset(self):\n",
    "        self.round = 0\n",
    "        self.invested = True\n",
    "        self.balance = self.initial_balance\n",
    "        self.price = self.get_price_for_round(self.round)\n",
    "        self.price_level = self.transform_price_to_price_level(self.price)\n",
    "        self.signal_sma = self.get_signal_sma(self.round)\n",
    "        return (self.price_level, self.signal_sma, self.invested)\n",
    "    \n",
    "    def get_next_state_and_reward(self, state, action):\n",
    "        # business logic\n",
    "        price_level, signal_sma, invested = state\n",
    "        result = dict()\n",
    "        if action == 'buy':\n",
    "            result['invested_in_next_step'] = True\n",
    "        elif action == 'sell':\n",
    "            result['invested_in_next_step'] = False\n",
    "        elif action == 'hold':\n",
    "            result['invested_in_next_step'] = invested\n",
    "        result['next_round'] = self.round + 1\n",
    "        \n",
    "        if invested:\n",
    "            result['absolute_change_of_balance'] = self.get_pct_change_for_round(\n",
    "                self.round) * self.balance\n",
    "        else:\n",
    "            result['absolute_change_of_balance'] = 0\n",
    "        \n",
    "        result['next_price'] = self.get_price_for_round(self.round)\n",
    "        result['next_price_level'] =  self.transform_price_to_price_level(result['next_price'])\n",
    "        result['signal_sma'] = self.get_signal_sma(self.round)\n",
    "        result['next_balance'] = self.balance + result['absolute_change_of_balance']\n",
    "        result['reward'] = result['absolute_change_of_balance']\n",
    "        return result\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        result = self.get_next_state_and_reward(\n",
    "            state=(self.price_level, self.signal_sma, self.invested),\n",
    "            action=action\n",
    "            )\n",
    "        self.round = result['next_round']\n",
    "        self.balance = result['next_balance']\n",
    "        self.invested = result['invested_in_next_step']\n",
    "        self.price = result['next_price']\n",
    "        self.price_level = result['next_price_level']\n",
    "        self.signal_sma = result['signal_sma']\n",
    "        state = (self.price_level, self.signal_sma, self.invested)\n",
    "        reward = result['reward']\n",
    "        done = self.is_terminal()\n",
    "        info = {'round': self.round, 'price': self.price, 'balance': self.balance}\n",
    "        return state, reward, done, info\n",
    "        \n",
    "    def get_price_for_round(self, round):\n",
    "        return self.asset.set_index('round').loc[round, 'price']\n",
    "    \n",
    "    def get_pct_change_for_round(self, round):\n",
    "        return self.asset.set_index('round').loc[round, 'pct_change']\n",
    "    \n",
    "    def get_signal_sma(self, round):\n",
    "        # +1: buy signal\n",
    "        # -1: sell signal\n",
    "        short_long_diff = self.asset.set_index('round').loc[round, 'short_long_diff']\n",
    "        if np.isnan(short_long_diff):\n",
    "            return np.nan\n",
    "        else:\n",
    "            return np.sign(short_long_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_policy(states):\n",
    "    \"\"\"\n",
    "    Defining a base policy. States are mapped to actions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    states : tuple\n",
    "        Environment states\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The resulting policy. Keys are states while the values are itself dictionaries composed \n",
    "        of action probability pairs.\n",
    "    \"\"\"\n",
    "    policy = {}\n",
    "    for state in states:\n",
    "        action_probability_pairs = dict()\n",
    "        # business logic\n",
    "        # signal sma could be positive, negative or nan\n",
    "        round, balance, price, signal_sma, invested = state\n",
    "        if np.isnan(signal_sma):\n",
    "            policy[state] = action_probability_pairs['hold'] = 1.0\n",
    "        else:\n",
    "            if invested & (signal_sma >= 0):\n",
    "                action_probability_pairs['sell'] = 0.2\n",
    "                action_probability_pairs['hold'] = 0.8\n",
    "            elif invested & (signal_sma < 0):\n",
    "                action_probability_pairs['sell'] = 0.8\n",
    "                action_probability_pairs['hold'] = 0.2\n",
    "            if (not invested) & (signal_sma >= 0):\n",
    "                action_probability_pairs['buy'] = 0.8\n",
    "                action_probability_pairs['hold'] = 0.2\n",
    "            elif (not invested) & (signal_sma < 0):\n",
    "                action_probability_pairs['buy'] = 0.2\n",
    "                action_probability_pairs['hold'] = 0.8\n",
    "        policy[state] = action_probability_pairs\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def choose_action(state, policy):\n",
    "    \"\"\"\n",
    "    Return an action based on the current policy and state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : tuple\n",
    "        Environment states \n",
    "    policy : dict\n",
    "        The resulting policy. Keys are states while the values are itself dictionaries composed \n",
    "        of action probability pairs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "        One of the investment options: buy, sell, hold\n",
    "    \"\"\"\n",
    "    action_probability_pairs = policy[state]\n",
    "    action = np.random.choice(\n",
    "        a=list(action_probability_pairs.keys()),\n",
    "        p=list(action_probability_pairs.values()),\n",
    "    )\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_policy(policy, num_episodes):\n",
    "    investmentGame = InvestmentGameSimple()\n",
    "    rewards = []\n",
    "    balances = []\n",
    "    actions = []\n",
    "    for i in range(num_episodes):\n",
    "        state = investmentGame.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_actions = []\n",
    "        while not done:\n",
    "            action = choose_action(state, policy)\n",
    "            state, reward, done, info = investmentGame.step(action)\n",
    "            episode_reward += reward\n",
    "            episode_actions.append(action)\n",
    "        rewards.append(episode_reward)\n",
    "        balances.append(info['balance'])\n",
    "        actions.append(episode_actions)\n",
    "    print(f\"Average balance: {np.mean(balances)}\")\n",
    "    return rewards, balances, actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewards, balances, actions = simulate_policy(policy, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_simulation_results = pd.DataFrame(actions, columns=[\n",
    "#                                      f\"action_in_round_{round}\" for round in range(InvestmentGameSimple().num_rounds)])\n",
    "\n",
    "# df_simulation_results['final_balance'] = balances\n",
    "# df_simulation_results[df_simulation_results['final_balance'] == df_simulation_results['final_balance'].max()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_actions(state):\n",
    "    invested = state[2]\n",
    "    if invested:\n",
    "        return ['sell', 'hold']\n",
    "    else:\n",
    "        return ['buy', 'hold']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_policy(states):\n",
    "    policy = dict()\n",
    "    for state in states:\n",
    "        valid_actions = get_valid_actions(state)\n",
    "        num_actions = len(valid_actions)\n",
    "        probability = 1 / num_actions\n",
    "        policy[state] = {action: probability for action in valid_actions}\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(env, policy):\n",
    "    \"\"\"\n",
    "    Return the trajectory for one episode under the given policy.\n",
    "    The trajectory is a list of state-action-reward pairs captured in every step of an episode. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : Environment class \n",
    "        Simulation environment\n",
    "    policy : dict\n",
    "        Mapping from state to action-probability dictionary\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of state-action-reward tuples\n",
    "    \"\"\"\n",
    "    trajectory = []\n",
    "    state = env.reset()\n",
    "    state_action_reward = [state] # initialized\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = choose_action(state, policy)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        state_action_reward.append(action)\n",
    "        state_action_reward.append(reward)\n",
    "        trajectory.append(state_action_reward)\n",
    "        state_action_reward = [state]\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eps_greedy(actions, eps, best_action):\n",
    "    action_probability_pairs = dict()\n",
    "    num_actions = len(actions)\n",
    "    for action in actions:\n",
    "        if action == best_action:\n",
    "            action_probability_pairs[action] = 1 - eps + eps / num_actions\n",
    "        else:\n",
    "            action_probability_pairs[action] = eps / num_actions\n",
    "    return action_probability_pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_policy_first_visit_mc(env, num_trajectories, eps, gamma):\n",
    "    # initialize a policy\n",
    "    states = env.state_space\n",
    "    policy = get_random_policy(states)\n",
    "    Q = {state: {action: 0 for action in get_valid_actions(state)} for state in states}\n",
    "    Q_n = {state: {action: 0 for action in get_valid_actions(state)} for state in states}\n",
    "    for iteration in range(num_trajectories):\n",
    "        # simulate trajectory\n",
    "        trajectory = get_trajectory(env, policy)\n",
    "        G = 0\n",
    "        T = len(trajectory) - 1\n",
    "        for t, state_action_reward in enumerate(reversed(trajectory)):\n",
    "            state, action, reward = state_action_reward\n",
    "            G = reward + gamma * G\n",
    "            first_visit = True\n",
    "            for j in range(T - t):\n",
    "                state_j = trajectory[j][0]\n",
    "                action_j = trajectory[j][1]\n",
    "                if (state, action) == (state_j, action_j):\n",
    "                    first_visit = False\n",
    "                if first_visit:\n",
    "                    Q[state][action] = Q[state][action] * Q_n[state][action] + G\n",
    "                    Q_n[state][action] += 1\n",
    "                    Q[state][action] /= Q_n[state][action]\n",
    "                    best_action = max(Q[state].items(), key=operator.itemgetter(1))[0]\n",
    "                    policy[state] = get_eps_greedy(get_valid_actions(state), eps, best_action)\n",
    "    return policy, Q, Q_n\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investmentGame = InvestmentGameSimple()\n",
    "# policy, Q, Q_n = on_policy_first_visit_mc(investmentGame, 100, 0.05, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = 'resources/data/policy.json'\n",
    "# with open(file_name, 'w') as fp:\n",
    "    # fp.write(ujson.dumps(policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-step Temporal-difference learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal difference methods can update a policy after a single or multiple state transitions. We are starting by defining the state-value function of a policy $\\pi$ for a one-step reward and the next state:\n",
    "$$\n",
    "v_{\\pi}(s) = E_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1})|S_t = s]\n",
    "$$\n",
    "\n",
    "*The idea in TD learning is that we use this observation to update the existing estimate by moving it in the direction of this new estimate.*\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-policy control with Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\hat{q}_{\\pi}(s,a) := \\hat{q}_{\\pi}(s,a) + \\alpha[r + \\gamma \\max \\hat{q}_{\\pi}(s', u) - \\hat{q}_{\\pi}(s,a)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the action the agent uses to update the action-value, u, is not necessarily the action that it will use for the next step. It is the action that maximizes the action-value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, gamma, eps, alpha, num_iter):\n",
    "    states = env.state_space\n",
    "    # initialize Q\n",
    "    Q = {state: {action: 0 for action in get_valid_actions(state)} for state in states}\n",
    "    # initialize a policy\n",
    "    policy = get_random_policy(states)\n",
    "    state = env.reset()\n",
    "    for iter in range(num_iter):\n",
    "        if (iter % 1000) == 0:\n",
    "            print(f\"Iteration: {iter}\")\n",
    "        best_action = max(Q[state].items(), key=operator.itemgetter(1))[0]\n",
    "        policy[state] = get_eps_greedy(get_valid_actions(state), eps, best_action)\n",
    "        action = choose_action(state, policy)\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        max_q = max(Q[state_next].values())\n",
    "        Q[state][action] += alpha * (reward + gamma * max_q - Q[state][action])\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "        else:\n",
    "            state = state_next\n",
    "        # strip policy\n",
    "        policy = {state: {max(policy[state].items(), key=operator.itemgetter(1))[\n",
    "            0]: 1} for state in states}\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "investmentGame = InvestmentGameSimple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 1000\n",
      "Iteration: 2000\n",
      "Iteration: 3000\n",
      "Iteration: 4000\n",
      "Iteration: 5000\n",
      "Iteration: 6000\n",
      "Iteration: 7000\n",
      "Iteration: 8000\n",
      "Iteration: 9000\n"
     ]
    }
   ],
   "source": [
    "policy, Q = q_learning(investmentGame, 1, 0.1, 0.01, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(4, -1, True): {'sell': 0.03801341865769722, 'hold': -0.002402239865334292},\n",
       " (4, -1, False): {'buy': 0.01762268124650116, 'hold': 0.0807708657198195},\n",
       " (4, 1, True): {'sell': 0.187980066256735, 'hold': 0.06003252787364823},\n",
       " (4, 1, False): {'buy': 0.1573246329495929, 'hold': 0.02924141074785779},\n",
       " (7, -1, True): {'sell': -0.023377641052261196, 'hold': -0.02755673315875193},\n",
       " (7, -1, False): {'buy': -0.005624337580907651, 'hold': 0.045709185081051845},\n",
       " (7, 1, True): {'sell': 0.07373333371973284, 'hold': 0.02705669075543443},\n",
       " (7, 1, False): {'buy': 0.06467519838296341, 'hold': 0.022967445493144993},\n",
       " (10, -1, True): {'sell': 0, 'hold': 0},\n",
       " (10, -1, False): {'buy': 0, 'hold': 0},\n",
       " (10, 1, True): {'sell': -0.0003300288361633123,\n",
       "  'hold': -0.002482873243326878},\n",
       " (10, 1, False): {'buy': 0.00257067298334964, 'hold': 0.021615746062951923}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f5cfaaaa153823abe9526a5efac16696eb8412c62d807a649f5b5fb84299378"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('rl-invest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
